{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-25T13:45:04.628587Z",
          "iopub.status.busy": "2024-02-25T13:45:04.627723Z",
          "iopub.status.idle": "2024-02-25T13:45:08.968328Z",
          "shell.execute_reply": "2024-02-25T13:45:08.967484Z",
          "shell.execute_reply.started": "2024-02-25T13:45:04.628526Z"
        },
        "id": "_jD16fhtzoea",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations import HorizontalFlip, Compose, Resize, Normalize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YIs-9Ko0gNG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_dir = \"/datasets/data/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htLiT68OERjV"
      },
      "outputs": [],
      "source": [
        "train_dir = drive_dir + \"/train\"\n",
        "\n",
        "train_img_dir = train_dir + \"/imgs\"\n",
        "train_mask_dir = train_dir + \"/masks\"\n",
        "\n",
        "train_imgs = list(sorted(os.listdir(train_img_dir)))\n",
        "train_masks = list(sorted(os.listdir(train_mask_dir)))\n",
        "\n",
        "train_val_ims = list(sorted(os.listdir(train_img_dir)))\n",
        "train_imgs, val_imgs = train_test_split(train_val_ims, test_size=0.2) #, random_state=42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fysu9YOGEXfP"
      },
      "outputs": [],
      "source": [
        "test_dir = drive_dir + \"/test\"\n",
        "\n",
        "test_img_dir = test_dir + \"/imgs\"\n",
        "\n",
        "test_imgs = list(sorted(os.listdir(test_img_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-25T13:45:08.999790Z",
          "iopub.status.busy": "2024-02-25T13:45:08.999093Z",
          "iopub.status.idle": "2024-02-25T13:45:09.010214Z",
          "shell.execute_reply": "2024-02-25T13:45:09.009148Z",
          "shell.execute_reply.started": "2024-02-25T13:45:08.999754Z"
        },
        "id": "7126RKqxzoeb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class SSDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, msk_dir, image_list, preprocessor, mode='train'):\n",
        "        self.transforms = transforms\n",
        "        self.imgs = image_list\n",
        "        self.img_dir, self.msk_dir = img_dir, msk_dir\n",
        "        self.labels = list(range(0, 13))\n",
        "        self.mode = mode\n",
        "        self.preprocessor = preprocessor\n",
        "        self.augmentation = transforms.Compose([\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == 'train' or self.mode == 'val':\n",
        "            file_image = f'train_{idx+1:04d}.png'\n",
        "        else:\n",
        "            file_image = f'{self.mode}_{idx+1:04d}.png'\n",
        "        train_img_dir = os.path.join(self.img_dir, file_image)\n",
        "        image = Image.open(train_img_dir).convert(\"RGB\")\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            image = self.augmentation(image)\n",
        "\n",
        "        image = transforms.ToTensor()(image)\n",
        "        image = image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        image = image.clamp(0, 1)\n",
        "\n",
        "        if self.msk_dir is not None:\n",
        "            if self.mode == 'train' or self.mode == 'val':\n",
        "                mask_image = f'train_{idx+1:04d}.png'\n",
        "            else:\n",
        "                mask_image = f'{self.mode}_{idx+1:04d}.png'\n",
        "            mask_path = os.path.join(self.msk_dir, mask_image)\n",
        "            mask = np.array(Image.open(mask_path).convert(\"RGB\"))[:,:,0]\n",
        "            input_dict = self.preprocessor.preprocess(images=image, segmentation_maps=mask, return_tensors='pt')\n",
        "        else:\n",
        "            input_dict = self.preprocessor.preprocess(images=image, return_tensors='pt')\n",
        "\n",
        "        for k, v in input_dict.items():\n",
        "          if isinstance(v, torch.Tensor):\n",
        "            input_dict[k].squeeze_()\n",
        "\n",
        "        return input_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-25T13:45:09.011967Z",
          "iopub.status.busy": "2024-02-25T13:45:09.011609Z",
          "iopub.status.idle": "2024-02-25T13:45:12.358181Z",
          "shell.execute_reply": "2024-02-25T13:45:12.357462Z",
          "shell.execute_reply.started": "2024-02-25T13:45:09.011939Z"
        },
        "id": "QmKPseY5zoeb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import OneFormerImageProcessor, OneFormerForUniversalSegmentation\n",
        "\n",
        "preprocessor = OneFormerImageProcessor.from_pretrained(\"shi-labs/oneformer_cityscapes_dinat_large\", num_text=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-25T13:45:12.905315Z",
          "iopub.status.busy": "2024-02-25T13:45:12.904986Z",
          "iopub.status.idle": "2024-02-25T13:45:12.911014Z",
          "shell.execute_reply": "2024-02-25T13:45:12.909997Z",
          "shell.execute_reply.started": "2024-02-25T13:45:12.905286Z"
        },
        "id": "pklAzqXjzoec",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Dataset\n",
        "train_dataset = SSDataset(train_img_dir, train_mask_dir, train_imgs, preprocessor=preprocessor, mode='train')\n",
        "val_dataset = SSDataset(train_img_dir, train_mask_dir, val_imgs, preprocessor=preprocessor, mode='val')\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "### Dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-25T14:06:30.960932Z",
          "iopub.status.busy": "2024-02-25T14:06:30.960272Z",
          "iopub.status.idle": "2024-02-25T14:06:31.328667Z",
          "shell.execute_reply": "2024-02-25T14:06:31.327897Z",
          "shell.execute_reply.started": "2024-02-25T14:06:30.960898Z"
        },
        "id": "gYnD_20Bzoec",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def Dice(pred,target):\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(pred * target)\n",
        "    y_sum = torch.sum(target * target)\n",
        "    z_sum = torch.sum(pred * pred)\n",
        "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    return loss\n",
        "\n",
        "def DiceLoss(pred, target):\n",
        "    return 1 - Dice(pred, target)\n",
        "\n",
        "### Criterion & Metric\n",
        "metric = Dice\n",
        "criterion = DiceLoss\n",
        "\n",
        "### Model Definition\n",
        "num_classes = 13\n",
        "device = 'cuda'\n",
        "model = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_cityscapes_dinat_large\",id2label = {i:i for i in range(num_classes)},\n",
        "                                                        label2id = {i:i for i in range(num_classes)}, ignore_mismatched_sizes=True).to(device)\n",
        "\n",
        "### Optim & Sceduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-25T14:06:33.215835Z",
          "iopub.status.busy": "2024-02-25T14:06:33.214883Z",
          "iopub.status.idle": "2024-02-25T14:38:46.499816Z",
          "shell.execute_reply": "2024-02-25T14:38:46.498704Z",
          "shell.execute_reply.started": "2024-02-25T14:06:33.215801Z"
        },
        "id": "ZE1Gr60Izoec",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "num_train_epochs=12\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_train_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n",
        "    loop = tqdm(train_dataloader, leave=True)\n",
        "    for batch in loop:\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "        targets = batch[\"labels\"].to(device) ### target label: 512x512\n",
        "        targets = targets.unsqueeze(1)\n",
        "\n",
        "        # one-hot encoding\n",
        "        one_hot_labels = torch.zeros(targets.size(0), 13,\n",
        "                                     targets.size(2), targets.size(3),\n",
        "                                     dtype=torch.float32, device=device)\n",
        "        one_hot_labels.scatter_(1, targets, 1) ### one_hot_labels output: 13x512x512\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(pixel_values)\n",
        "        logits = outputs.logits ### Model output: 128x128\n",
        "        logits = F.interpolate(logits, scale_factor=4, mode='bilinear')\n",
        "\n",
        "        loss = criterion(logits, one_hot_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_dice = 0.0\n",
        "        total_samples = 0\n",
        "        loop = tqdm(val_dataloader, leave=True)\n",
        "        for batch in loop:\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "            targets = batch[\"labels\"].to(device) ### target label: 512x512\n",
        "            targets = targets.unsqueeze(1)\n",
        "\n",
        "            # one-hot encoding\n",
        "            one_hot_labels = torch.zeros(targets.size(0), 13,\n",
        "                                     targets.size(2), targets.size(3),\n",
        "                                     dtype=torch.float32, device=device)\n",
        "            one_hot_labels.scatter_(1, targets, 1)\n",
        "\n",
        "            outputs = model(pixel_values)\n",
        "            logits = outputs.logits ### Model output: 128x128\n",
        "            logits = F.interpolate(logits, scale_factor=4, mode='bilinear')[0]\n",
        "\n",
        "            total_dice += metric(logits, one_hot_labels).item()\n",
        "            total_samples += 1\n",
        "            avg_dice = total_dice / total_samples\n",
        "\n",
        "        print(f\"Validation Dice Score: {avg_dice:.4f}\")\n",
        "\n",
        "    model.train()\n",
        "    lr_scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFE194v0zoed"
      },
      "source": [
        "# Compute Metric : DICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-25T14:49:10.604336Z",
          "iopub.status.busy": "2024-02-25T14:49:10.604001Z",
          "iopub.status.idle": "2024-02-25T14:49:10.610314Z",
          "shell.execute_reply": "2024-02-25T14:49:10.609318Z",
          "shell.execute_reply.started": "2024-02-25T14:49:10.604310Z"
        },
        "id": "syByL1Khzoed",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def rle_encode(mask_image):\n",
        "    pixels = mask_image.flatten()\n",
        "    pixels[0] = 0\n",
        "    pixels[-1] = 0\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
        "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
        "    return runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0qhKn1Wzoed"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-25T14:49:13.963451Z",
          "iopub.status.busy": "2024-02-25T14:49:13.963070Z",
          "iopub.status.idle": "2024-02-25T14:50:46.457356Z",
          "shell.execute_reply": "2024-02-25T14:50:46.456289Z",
          "shell.execute_reply.started": "2024-02-25T14:49:13.963416Z"
        },
        "id": "vagyWyTzzoed",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "###batch_size=1, shuffle=False\n",
        "test_dataset = SSDataset(test_img_dir, None, test_imgs, preprocessor, \"test\")\n",
        "test_dataloader = DataLoader(test_dataset,batch_size = 1, shuffle = False)\n",
        "test_model = model\n",
        "test_model.eval()\n",
        "\n",
        "\n",
        "out_dict = []\n",
        "output = []\n",
        "inputs = []\n",
        "\n",
        "for i,data in tqdm(enumerate(test_dataloader)):\n",
        "    image = data.pixel_values.to(device)\n",
        "    prediction = test_model(image)\n",
        "\n",
        "    prediction = F.interpolate(prediction.logits, size=(600,800), mode='bilinear').argmax(dim=1).unsqueeze(0)\n",
        "    prediction = prediction.squeeze(0,1)\n",
        "    ##prediction shape: [1,600,800]\n",
        "\n",
        "    mask_labels = []\n",
        "    image_name = test_imgs[i].rsplit('.')[0]\n",
        "\n",
        "    # RLE encoding\n",
        "    for j in range(13):\n",
        "        mask_label = torch.zeros(prediction.shape)\n",
        "        mask_label[prediction==j]=1\n",
        "\n",
        "        mask_labels.append(mask_label)\n",
        "\n",
        "    for j in range(0,13):\n",
        "        mask_label = mask_labels[j].squeeze().numpy()\n",
        "        encode = rle_encode(mask_label)\n",
        "        out_dict.append((f'{image_name}_{j}', ' '.join(str(_) for _ in encode)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## create csv\n",
        "import pandas as pdb\n",
        "\n",
        "df = pdb.DataFrame(out_dict)\n",
        "df.columns=['ImageId','EncodedPixels']\n",
        "df=df.set_index('ImageId')\n",
        "\n",
        "df.to_csv('/kaggle/working/result.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 5119625,
          "sourceId": 70477,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30513,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
