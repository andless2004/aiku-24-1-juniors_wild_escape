{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T05:39:09.575594Z","iopub.status.busy":"2024-07-02T05:39:09.575233Z","iopub.status.idle":"2024-07-02T05:39:09.584605Z","shell.execute_reply":"2024-07-02T05:39:09.583614Z","shell.execute_reply.started":"2024-07-02T05:39:09.575558Z"},"trusted":true},"outputs":[],"source":["import os\n","import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","import cv2\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import StepLR\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision.transforms as transforms\n","from torchvision.transforms import ToTensor, ToPILImage\n","from torchvision.transforms.functional import to_tensor, to_pil_image\n","\n","import albumentations as A\n","from albumentations import HorizontalFlip, Compose, Resize, Normalize\n","from albumentations.pytorch import ToTensorV2\n","\n","from tqdm import tqdm\n","\n","from PIL import Image, ImageOps, ImageEnhance\n","\n","import segmentation_models_pytorch as smp\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T05:39:09.586581Z","iopub.status.busy":"2024-07-02T05:39:09.585909Z","iopub.status.idle":"2024-07-02T05:39:09.602234Z","shell.execute_reply":"2024-07-02T05:39:09.601278Z","shell.execute_reply.started":"2024-07-02T05:39:09.586543Z"},"trusted":true},"outputs":[],"source":["drive_dir = \"/kaggle/input/4-aikuthon/data/data\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T05:39:09.605175Z","iopub.status.busy":"2024-07-02T05:39:09.604904Z","iopub.status.idle":"2024-07-02T05:39:09.627779Z","shell.execute_reply":"2024-07-02T05:39:09.627066Z","shell.execute_reply.started":"2024-07-02T05:39:09.605151Z"},"trusted":true},"outputs":[],"source":["train_dir = drive_dir + \"/train\"\n","\n","train_img_dir = train_dir + \"/imgs\"\n","train_mask_dir = train_dir + \"/masks\"\n","\n","train_imgs = list(sorted(os.listdir(train_img_dir)))\n","train_masks = list(sorted(os.listdir(train_mask_dir)))\n","\n","train_val_ims = list(sorted(os.listdir(train_img_dir)))\n","train_imgs, val_imgs = train_test_split(train_val_ims, test_size=0.2, random_state=42) #"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T05:39:09.629124Z","iopub.status.busy":"2024-07-02T05:39:09.628831Z","iopub.status.idle":"2024-07-02T05:39:09.634433Z","shell.execute_reply":"2024-07-02T05:39:09.633605Z","shell.execute_reply.started":"2024-07-02T05:39:09.629100Z"},"trusted":true},"outputs":[],"source":["test_dir = drive_dir + \"/test\"\n","\n","test_img_dir = test_dir + \"/imgs\"\n","\n","test_imgs = list(sorted(os.listdir(test_img_dir)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T05:39:09.635902Z","iopub.status.busy":"2024-07-02T05:39:09.635623Z","iopub.status.idle":"2024-07-02T05:39:09.650555Z","shell.execute_reply":"2024-07-02T05:39:09.649806Z","shell.execute_reply.started":"2024-07-02T05:39:09.635878Z"},"trusted":true},"outputs":[],"source":["class SSDataset(torch.utils.data.Dataset):\n","    def __init__(self, img_dir, msk_dir, image_list, preprocessor, mode='train'):\n","        self.transforms = transforms\n","        self.imgs = image_list\n","        self.img_dir, self.msk_dir = img_dir, msk_dir\n","        self.labels = list(range(0, 13))\n","        self.mode = mode\n","        self.preprocessor = preprocessor\n","        self.augmentation = transforms.Compose([\n","            transforms.RandomVerticalFlip(p=0.55),\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n","        ])\n","        self.preprocessing = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def __getitem__(self, idx):\n","        if self.mode == 'train' or self.mode == 'val':\n","            file_image = f'train_{idx+1:04d}.png'\n","        else: \n","            file_image = f'{self.mode}_{idx+1:04d}.png'\n","        train_img_dir = os.path.join(self.img_dir, file_image)\n","        image = Image.open(train_img_dir).convert(\"RGB\")\n","    \n","        if self.mode == 'train':\n","            image = self.augmentation(image)\n","        \n","        image = self.preprocessing(image)\n","    \n","        if self.msk_dir is not None:\n","            if self.mode == 'train' or self.mode == 'val':\n","                mask_image = f'train_{idx+1:04d}.png'\n","            else:\n","                mask_image = f'{self.mode}_{idx+1:04d}.png'\n","            mask_path = os.path.join(self.msk_dir, mask_image)\n","            mask = np.array(Image.open(mask_path).convert(\"RGB\"))[:,:,0]\n","            input_dict = self.preprocessor.preprocess(images=image, segmentation_maps=mask, return_tensors='pt')\n","        else:\n","            input_dict = self.preprocessor.preprocess(images=image, return_tensors='pt')\n","\n","        for k, v in input_dict.items():\n","            input_dict[k].squeeze_()\n","\n","        return input_dict\n","\n","    def __len__(self):\n","        return len(self.imgs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T05:39:09.651908Z","iopub.status.busy":"2024-07-02T05:39:09.651565Z","iopub.status.idle":"2024-07-02T05:39:09.664981Z","shell.execute_reply":"2024-07-02T05:39:09.664121Z","shell.execute_reply.started":"2024-07-02T05:39:09.651875Z"},"trusted":true},"outputs":[],"source":["from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n","preprocessor = SegformerImageProcessor(\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T05:39:09.666406Z","iopub.status.busy":"2024-07-02T05:39:09.666081Z","iopub.status.idle":"2024-07-02T05:39:09.675216Z","shell.execute_reply":"2024-07-02T05:39:09.674378Z","shell.execute_reply.started":"2024-07-02T05:39:09.666368Z"},"trusted":true},"outputs":[],"source":["### Dataset\n","train_dataset = SSDataset(train_img_dir, train_mask_dir, train_imgs, preprocessor=preprocessor, mode='train')\n","val_dataset = SSDataset(train_img_dir, train_mask_dir, val_imgs, preprocessor=preprocessor, mode='val')\n","\n","### Dataloader\n","train_dataloader = DataLoader(train_dataset, batch_size = 4, shuffle = True)\n","val_dataloader = DataLoader(val_dataset, batch_size = 4, shuffle = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T05:39:09.676636Z","iopub.status.busy":"2024-07-02T05:39:09.676319Z","iopub.status.idle":"2024-07-02T05:39:12.191495Z","shell.execute_reply":"2024-07-02T05:39:12.190478Z","shell.execute_reply.started":"2024-07-02T05:39:09.676603Z"},"trusted":true},"outputs":[],"source":["from transformers import SegformerForSemanticSegmentation\n","\n","def Dice(pred,target):\n","    target = target.float()\n","    smooth = 1e-5\n","    intersect = torch.sum(pred * target)\n","    y_sum = torch.sum(target * target)\n","    z_sum = torch.sum(pred * pred)\n","    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n","    return loss\n","\n","def DiceLoss(pred, target):\n","    return 1 - Dice(pred, target)\n","\n","### Criterion & Metric\n","metric = Dice\n","criterion = DiceLoss\n","\n","### Model Definition\n","num_classes = 13\n","device = 'cuda'\n","model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\",id2label = {i:i for i in range(num_classes)},\n","                                                        label2id = {i:i for i in range(num_classes)}, ignore_mismatched_sizes=True).to(device)\n","\n","### Optim & Sceduler\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n","lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T05:39:12.193184Z","iopub.status.busy":"2024-07-02T05:39:12.192887Z","iopub.status.idle":"2024-07-02T05:43:09.125549Z","shell.execute_reply":"2024-07-02T05:43:09.124319Z","shell.execute_reply.started":"2024-07-02T05:39:12.193159Z"},"trusted":true},"outputs":[],"source":["model.train()\n","num_train_epochs=15\n","\n","# Training loop\n","for epoch in range(num_train_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_train_epochs}\")\n","    loop = tqdm(train_dataloader, leave=True)\n","    for batch in loop:\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","\n","        targets = batch[\"labels\"].to(device) ### target label: 512x512\n","        targets = targets.unsqueeze(1)\n","\n","        # one-hot encoding\n","        one_hot_labels = torch.zeros(targets.size(0), 13,\n","                                     targets.size(2), targets.size(3),\n","                                     dtype=torch.float32, device=device)\n","        one_hot_labels.scatter_(1, targets, 1) ### one_hot_labels output: 13x512x512\n","\n","        optimizer.zero_grad()\n","        outputs = model(pixel_values)\n","        logits = outputs.logits ### Model output: 128x128\n","        logits = F.interpolate(logits, scale_factor=4, mode='bilinear')\n","        \n","        loss = criterion(logits, one_hot_labels)\n","        loss.backward()\n","        optimizer.step()\n","        loop.set_postfix(loss=loss.item())\n","\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        total_dice = 0.0\n","        total_samples = 0\n","        loop = tqdm(val_dataloader, leave=True)\n","        for batch in loop:\n","            pixel_values = batch[\"pixel_values\"].to(device)\n","\n","            targets = batch[\"labels\"].to(device) ### target label: 512x512\n","            targets = targets.unsqueeze(1)\n","\n","            # one-hot encoding\n","            one_hot_labels = torch.zeros(targets.size(0), 13,\n","                                     targets.size(2), targets.size(3),\n","                                     dtype=torch.float32, device=device)\n","            one_hot_labels.scatter_(1, targets, 1)\n","\n","            outputs = model(pixel_values)\n","            logits = outputs.logits ### Model output: 128x128\n","            logits = F.interpolate(logits, scale_factor=4, mode='bilinear')[0]\n","\n","            total_dice += metric(logits, one_hot_labels).item()\n","            total_samples += 1\n","            avg_dice = total_dice / total_samples\n","\n","        print(f\"Validation Dice Score: {avg_dice:.4f}\")\n","\n","    model.train()\n","    lr_scheduler.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:43:09.126469Z","iopub.status.idle":"2024-07-02T05:43:09.126818Z","shell.execute_reply":"2024-07-02T05:43:09.126666Z","shell.execute_reply.started":"2024-07-02T05:43:09.126651Z"},"trusted":true},"outputs":[],"source":["def rle_encode(mask_image):\n","    pixels = mask_image.flatten()\n","    pixels[0] = 0\n","    pixels[-1] = 0\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n","    runs[1::2] = runs[1::2] - runs[:-1:2]\n","    return runs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:43:09.128486Z","iopub.status.idle":"2024-07-02T05:43:09.128801Z","shell.execute_reply":"2024-07-02T05:43:09.128662Z","shell.execute_reply.started":"2024-07-02T05:43:09.128648Z"},"trusted":true},"outputs":[],"source":["test_dataset = SSDataset('/kaggle/input/4-aikuthon/data/data/test/imgs', None, test_imgs, preprocessor, \"test\")\n","test_dataloader = DataLoader(test_dataset,batch_size = 1, shuffle = False)\n","test_model = model\n","test_model.eval()\n","\n","\n","out_dict = []\n","output = []\n","inputs = []\n","\n","for i,data in tqdm(enumerate(test_dataloader)):\n","    image = data.pixel_values.to(device)\n","    prediction = test_model(image)\n","    \n","    prediction = F.interpolate(prediction.logits, size=(600,800), mode='bilinear').argmax(dim=1).unsqueeze(0)\n","    prediction = prediction.squeeze(0,1)\n","    ##prediction shape: [1,600,800]\n","\n","    mask_labels = []\n","    image_name = test_imgs[i].rsplit('.')[0]\n","\n","    ##RLE encoding\n","    for j in range(13):\n","        mask_label = torch.zeros(prediction.shape)\n","        mask_label[prediction==j]=1\n","\n","        mask_labels.append(mask_label)\n","\n","    for j in range(0,13):\n","        mask_label = mask_labels[j].squeeze().numpy()\n","        encode = rle_encode(mask_label)\n","        out_dict.append((f'{image_name}_{j}', ' '.join(str(_) for _ in encode)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## create csv\n","import pandas as pdb\n","\n","df = pdb.DataFrame(out_dict)\n","df.columns=['ImageId','EncodedPixels']\n","df=df.set_index('ImageId')\n","\n","df.to_csv('/kaggle/working/submission.csv')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7710298,"sourceId":70477,"sourceType":"competition"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
